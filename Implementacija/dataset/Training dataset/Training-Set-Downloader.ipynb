{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Downloading and storing 8000 tweets (6491) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[############                  ] | ETA: 00:01:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854867673993359360\n",
      "3249\n"
     ]
    }
   ],
   "source": [
    "import twython as tw\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import os\n",
    "\n",
    "#twitter api credentials\n",
    "CONSUMER_KEY = \"qGWa0auDtpVJXx1LaZQspsVWe\"\n",
    "CONSUMER_SECRET = \"scSr5x4R1u7w7KAoQ47J8xG1J0nUozhpowzzjNPO2DYuD89plg\"\n",
    "OAUTH_TOKEN = \"846031238813171713-5ywVPWAEPNDBlVnO6ReosjvAcTn5K3v\"\n",
    "OAUTH_TOKEN_SECRET = \"1itTLCn9ytm6L0UNeHWObgaofsk4qE62cHyHvxSa1nBRH\"\n",
    "twitter = tw.Twython(\n",
    "    CONSUMER_KEY, CONSUMER_SECRET,\n",
    "    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "i = 0\n",
    "pbar = pyprind.ProgBar(8000 - i)\n",
    "\n",
    "maxid = 0\n",
    "#maxid = 849024394470899713\n",
    "\n",
    "while i<8000:\n",
    "    try:\n",
    "        if maxid == 0 :\n",
    "            search = twitter.search(q='#sarcasm -filter:retweets AND -filter:replies', result_type='recent', lang='en', count=100)\n",
    "        else:\n",
    "            search = twitter.search(q='#sarcasm -filter:retweets AND -filter:replies', result_type='recent', lang='en', count=100, max_id=maxid)\n",
    "\n",
    "\n",
    "        tweets = search['statuses']\n",
    "\n",
    "        tid = \"\"\n",
    "        for tweet in tweets:\n",
    "            df = df.append([[tweet['text'].replace(\"\\n\", \"\")]], ignore_index=True)\n",
    "            i += 1\n",
    "            tid = tweet['id']\n",
    "            #print tweet['id'], tweet['text'], '\\n\\n\\n'\n",
    "            pbar.update()\n",
    "        try:\n",
    "            maxid = int(tid)\n",
    "        except:\n",
    "            print tid\n",
    "    except:\n",
    "        break\n",
    "\n",
    "print maxid\n",
    "print i\n",
    "df.columns = ['tweet']\n",
    "df.to_csv('./training_set_f.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Eliminating duplicates from downloaded tweets (3264 -> 3017) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -u training_set_f.csv > training_set_f2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3017\r\n"
     ]
    }
   ],
   "source": [
    "!grep -r \".*\" training_set_f2.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Extracting implicit phrases**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n",
      "#sarcasm\n",
      ":)\n",
      "#sarcasm\n",
      "{(u'nezna', u'programirati', u','): 1, (u\"'\", u'gluposti', u'i'): 1, (u'gluposti', u'i', u'sramoti'): 1, (u'pa', u'pishe', u\"'\"): 1, (u',', u'pa', u'pishe'): 1, (u'pishe', u\"'\", u'gluposti'): 1, (u'i', u'sramoti', u'se'): 1, (u'Mihael', u'nezna', u'programirati'): 1, (u'programirati', u',', u'pa'): 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def preprocessor(text):\n",
    "    #text = re.sub(\"#[^\\s+]+\", \"\", text)\n",
    "    #return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()\n",
    "    tt = TweetTokenizer()\n",
    "    tokenized = tt.tokenize(text)\n",
    "    tokenized_f = []\n",
    "    for token in tokenized:\n",
    "        if any(special in token for special in [\"#\", \":)\", \":-)\", \":(\", \":/\", \":P\"]):\n",
    "            print token\n",
    "        else:\n",
    "            tokenized_f.append(token)\n",
    "    return tokenized_f\n",
    "\n",
    "def extract_n_grams(sentence, lowerbound=3, upperbound=10):\n",
    "    lm = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "    for n in range(lowerbound, upperbound + 1):\n",
    "        ngram = ngrams(sentence, n)\n",
    "        for item in ngram:\n",
    "            lm[n][item] = lm[n].get(item, 0) + 1\n",
    "    return lm\n",
    "    \n",
    "niz1 = \"Mihael nezna programirati, pa pishe' gluposti i sramoti se:) #sarcasm\"\n",
    "preprocessor(niz1)\n",
    "print extract_n_grams(preprocessor(niz1))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "import pyprind\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"#[^\\s+]+\", \"\", text)\n",
    "    return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()\n",
    "\n",
    "def extract_n_grams(phrases, sentence, lowerbound=3, upperbound=10):\n",
    "    for n in range(lowerbound, upperbound + 1):\n",
    "        ngram = ngrams(sentence, n)\n",
    "        for item in ngram:\n",
    "            phrases[n][item] = phrases[n].get(item, 0) + 1\n",
    "    return phrases\n",
    "#Intialiaze upper and lower bound\n",
    "lowerbound = 3\n",
    "upperbound = 10\n",
    "\n",
    "#initialize dict with phrases\n",
    "phrases = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "\n",
    "def extract_n_phrases(training_set_file, progress_bar=True):\n",
    "    if progress_bar:\n",
    "        tset_len = sum(1 for row in open(training_set_file, 'r'))\n",
    "    with open(training_set_file, 'r') as training_set:\n",
    "        if progress_bar:\n",
    "            pbar = pyprind.ProgBar(tset_len)\n",
    "        for tweet in training_set:\n",
    "            extract_n_grams(phrases, preprocessor(tweet))\n",
    "            #print tweet, preprocessor(tweet)\n",
    "            #print extract_n_grams(phrases, preprocessor(tweet))[3]\n",
    "            #for e in extract_n_grams(phrases, preprocessor(tweet))[3]:\n",
    "            #    print extract_n_grams(phrases, preprocessor(tweet))[3][e]\n",
    "            #    break\n",
    "            if progress_bar:\n",
    "                pbar.update()\n",
    "\n",
    "extract_n_phrases('./training_set_f2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. Extract phrases that occur at least thrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases_filtered = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "for n_gram in range(lowerbound, upperbound + 1):\n",
    "    for phrase in phrases[n_gram]:\n",
    "        if phrases[n_gram][phrase] >= 3:\n",
    "            phrases_filtered[n_gram][phrase] = phrases[n_gram][phrase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. Store n-grams to file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_phrases = open('./grams/n_grams.pkl', 'wb')\n",
    "pickle.dump(phrases_filtered, output_phrases)\n",
    "output_phrases.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. Convert n-grams to phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases_converted = []\n",
    "for n_gram in range(lowerbound, upperbound + 1):\n",
    "    for phrase in phrases_filtered[n_gram]:\n",
    "        key = ' '.join(phrase)\n",
    "        phrases_converted.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>don t say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it will be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking forward to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t see that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I really want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you don t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>look like a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if I m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love it when</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               phrase\n",
       "0           don t say\n",
       "1          it will be\n",
       "2  looking forward to\n",
       "3          t see that\n",
       "4       Thank you for\n",
       "5       I really want\n",
       "6           you don t\n",
       "7         look like a\n",
       "8              if I m\n",
       "9        love it when"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for phrase in phrases_converted:\n",
    "    df = df.append([phrase], ignore_index=True)\n",
    "\n",
    "df.columns = ['phrase']\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases: 505\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print \"Number of phrases:\", len(phrases_converted)\n",
    "\n",
    "output_phrases = open('./grams/phrases.pkl', 'wb')\n",
    "pickle.dump(phrases_converted, output_phrases)\n",
    "output_phrases.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
