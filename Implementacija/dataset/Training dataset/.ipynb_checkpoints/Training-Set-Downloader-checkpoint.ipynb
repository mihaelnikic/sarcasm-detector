{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Downloading and storing 8000 tweets (6491) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849024394470899713\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "import twython as tw\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import os\n",
    "\n",
    "#twitter api credentials\n",
    "CONSUMER_KEY = \"qGWa0auDtpVJXx1LaZQspsVWe\"\n",
    "CONSUMER_SECRET = \"scSr5x4R1u7w7KAoQ47J8xG1J0nUozhpowzzjNPO2DYuD89plg\"\n",
    "OAUTH_TOKEN = \"846031238813171713-5ywVPWAEPNDBlVnO6ReosjvAcTn5K3v\"\n",
    "OAUTH_TOKEN_SECRET = \"1itTLCn9ytm6L0UNeHWObgaofsk4qE62cHyHvxSa1nBRH\"\n",
    "twitter = tw.Twython(\n",
    "    CONSUMER_KEY, CONSUMER_SECRET,\n",
    "    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "i = 0\n",
    "pbar = pyprind.ProgBar(8000 - i)\n",
    "\n",
    "\n",
    "maxid = 849024394470899713\n",
    "\n",
    "while i<8000:\n",
    "    try:\n",
    "        if maxid == 0 :\n",
    "            search = twitter.search(q='#sarcasm', result_type='recent', lang='en', count=100)\n",
    "        else:\n",
    "            search = twitter.search(q='#sarcasm', result_type='recent', lang='en', count=100, max_id=maxid)\n",
    "\n",
    "\n",
    "        tweets = search['statuses']\n",
    "\n",
    "        tid = \"\"\n",
    "        for tweet in tweets:\n",
    "            df = df.append([[tweet['text'].replace(\"\\n\", \"\")]], ignore_index=True)\n",
    "            i += 1\n",
    "            tid = tweet['id']\n",
    "            #print tweet['id'], tweet['text'], '\\n\\n\\n'\n",
    "            pbar.update()\n",
    "        try:\n",
    "            maxid = int(tid)\n",
    "        except:\n",
    "            print tid\n",
    "    except:\n",
    "        break\n",
    "\n",
    "print maxid\n",
    "print i\n",
    "df.columns = ['tweet']\n",
    "df.to_csv('./training_set_3.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Eliminating duplicates from downloaded tweets (6491 -> 5469) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -u training_set.csv > training_set_filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458\r\n"
     ]
    }
   ],
   "source": [
    "!grep -r \".*\" training_set_filtered.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Extracting implicit phrases**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('nezna', 'programirati', 'pa'): 1, ('pishe', 'gluposti', 'i'): 1, ('pa', 'pishe', 'gluposti'): 1, ('programirati', 'pa', 'pishe'): 1, ('i', 'sramoti', 'se'): 1, ('Mihael', 'nezna', 'programirati'): 1, ('gluposti', 'i', 'sramoti'): 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"#[^\\s+]+\", \"\", text)\n",
    "    return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()\n",
    "\n",
    "def extract_n_grams(sentence, lowerbound=3, upperbound=10):\n",
    "    lm = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "    for n in range(lowerbound, upperbound + 1):\n",
    "        ngram = ngrams(sentence, n)\n",
    "        for item in ngram:\n",
    "            lm[n][item] = lm[n].get(item, 0) + 1\n",
    "    return lm\n",
    "    \n",
    "niz1 = \"Mihael nezna programirati, pa pishe gluposti i sramoti se:) #sarcasm\"\n",
    "preprocessor(niz1)\n",
    "print extract_n_grams(preprocessor(niz1))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "import pyprind\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"#[^\\s+]+\", \"\", text)\n",
    "    return re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()\n",
    "\n",
    "def extract_n_grams(phrases, sentence, lowerbound=3, upperbound=10):\n",
    "    for n in range(lowerbound, upperbound + 1):\n",
    "        ngram = ngrams(sentence, n)\n",
    "        for item in ngram:\n",
    "            phrases[n][item] = phrases[n].get(item, 0) + 1\n",
    "    return phrases\n",
    "#Intialiaze upper and lower bound\n",
    "lowerbound = 3\n",
    "upperbound = 10\n",
    "\n",
    "#initialize dict with phrases\n",
    "phrases = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "\n",
    "def extract_n_phrases(training_set_file, progress_bar=True):\n",
    "    if progress_bar:\n",
    "        tset_len = sum(1 for row in open(training_set_file, 'r'))\n",
    "    with open(training_set_file, 'r') as training_set:\n",
    "        if progress_bar:\n",
    "            pbar = pyprind.ProgBar(tset_len)\n",
    "        for tweet in training_set:\n",
    "            extract_n_grams(phrases, preprocessor(tweet))\n",
    "            #print tweet, preprocessor(tweet)\n",
    "            #print extract_n_grams(phrases, preprocessor(tweet))[3]\n",
    "            #for e in extract_n_grams(phrases, preprocessor(tweet))[3]:\n",
    "            #    print extract_n_grams(phrases, preprocessor(tweet))[3][e]\n",
    "            #    break\n",
    "            if progress_bar:\n",
    "                pbar.update()\n",
    "\n",
    "extract_n_phrases('./training_set_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. Extract phrases that occur at least thrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases_filtered = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "for n_gram in range(lowerbound, upperbound + 1):\n",
    "    for phrase in phrases[n_gram]:\n",
    "        if phrases[n_gram][phrase] >= 3:\n",
    "            phrases_filtered[n_gram][phrase] = phrases[n_gram][phrase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. Store n-grams to file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_phrases = open('./grams/n_grams.pkl', 'wb')\n",
    "pickle.dump(phrases_filtered, output_phrases)\n",
    "output_phrases.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. Convert n-grams to phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases_converted = {n:dict() for n in range(lowerbound, upperbound + 1)}\n",
    "for n_gram in range(lowerbound, upperbound + 1):\n",
    "    for phrase in phrases_filtered[n_gram]:\n",
    "        key = ' '.join(phrase)\n",
    "        phrases_converted[n_gram][key] = phrases_filtered[n_gram][phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "output_phrases = open('./grams/phrases.pkl', 'wb')\n",
    "pickle.dump(phrases_converted, output_phrases)\n",
    "output_phrases.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
